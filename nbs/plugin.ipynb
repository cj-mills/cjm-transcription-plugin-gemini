{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9385067",
   "metadata": {},
   "source": [
    "# Gemini Plugin\n",
    "\n",
    "> Plugin implementation for Google Gemini API transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3341181",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c5aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a2bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import os\n",
    "import logging\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional, List, Union, Tuple\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from fastcore.basics import patch\n",
    "\n",
    "try:\n",
    "    from google import genai\n",
    "    from google.genai import types\n",
    "    GEMINI_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GEMINI_AVAILABLE = False\n",
    "\n",
    "try:\n",
    "    from cjm_ffmpeg_utils.core import FFMPEG_AVAILABLE\n",
    "    from cjm_ffmpeg_utils.audio import downsample_audio\n",
    "except ImportError:\n",
    "    FFMPEG_AVAILABLE = False\n",
    "    \n",
    "# Import domain-specific plugin interface from migrated system\n",
    "from cjm_transcription_plugin_system.plugin_interface import TranscriptionPlugin\n",
    "from cjm_transcription_plugin_system.core import AudioData, TranscriptionResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfcb260-d962-49ae-8bb9-d521a723f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class GeminiPlugin(TranscriptionPlugin):\n",
    "    \"\"\"Google Gemini API transcription plugin.\"\"\"\n",
    "    \n",
    "    # Default audio-capable models (can be overridden)\n",
    "    DEFAULT_AUDIO_MODELS = [\n",
    "        \"gemini-2.5-flash\",\n",
    "        \"gemini-2.5-flash-preview-05-20\",\n",
    "        \"gemini-2.5-pro\",\n",
    "        \"gemini-2.5-pro-preview-05-06\",\n",
    "        \"gemini-2.0-flash\",\n",
    "        \"gemini-2.0-flash-exp\",\n",
    "        \"gemini-1.5-flash\",\n",
    "        \"gemini-1.5-flash-latest\",\n",
    "        \"gemini-1.5-pro\",\n",
    "        \"gemini-1.5-pro-latest\",\n",
    "    ]\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the Gemini plugin with default configuration.\"\"\"\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{type(self).__name__}\")\n",
    "        self.config = {}\n",
    "        self.client = None\n",
    "        self.available_models = []\n",
    "        self.model_token_limits = {}  # Store model name -> output_token_limit mapping\n",
    "        self.uploaded_files = []  # Track uploaded files for cleanup\n",
    "    \n",
    "    @property\n",
    "    def name(\n",
    "        self\n",
    "    ) -> str:  # Returns the plugin name identifier\n",
    "        \"\"\"Return the plugin name identifier.\"\"\"\n",
    "        return \"gemini\"\n",
    "    \n",
    "    @property\n",
    "    def version(\n",
    "        self\n",
    "    ) -> str:  # Returns the plugin version string\n",
    "        \"\"\"Return the plugin version string.\"\"\"\n",
    "        return \"1.0.0\"\n",
    "    \n",
    "    @property\n",
    "    def supported_formats(\n",
    "        self\n",
    "    ) -> List[str]:  # Returns list of supported audio formats\n",
    "        \"\"\"Return list of supported audio file formats.\"\"\"\n",
    "        return [\"wav\", \"mp3\", \"aiff\", \"aac\", \"ogg\", \"flac\"]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_config_schema(\n",
    "        current_model: str=\"gemini-2.5-flash\",\n",
    "        max_tokens: int=65536,\n",
    "        available_models: List[str]=None\n",
    "    ) -> Dict[str, Any]:  # Returns JSON schema for configuration validation\n",
    "        \"\"\"Return configuration schema for Gemini.\"\"\"\n",
    "        \n",
    "        if not available_models:\n",
    "            available_models=GeminiPlugin.DEFAULT_AUDIO_MODELS\n",
    "        \n",
    "        return {\n",
    "            \"$schema\": \"http://json-schema.org/draft-07/schema#\",\n",
    "            \"type\": \"object\",\n",
    "            \"title\": \"Gemini Configuration\",\n",
    "            \"properties\": {\n",
    "                \"model\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"default\": \"gemini-2.5-flash\",\n",
    "                    \"description\": \"Gemini model to use for transcription\",\n",
    "                    \"enum\": available_models\n",
    "                },\n",
    "                \"api_key\": {\n",
    "                    \"type\": [\"string\", \"null\"],\n",
    "                    \"default\": None,\n",
    "                    \"description\": \"Google API key (defaults to GEMINI_API_KEY env var)\"\n",
    "                },\n",
    "                \"prompt\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"default\": \"Generate a transcription of the audio, only extract speech and ignore background audio.\",\n",
    "                    \"description\": \"Prompt for transcription\"\n",
    "                },\n",
    "                \"temperature\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"minimum\": 0.0,\n",
    "                    \"maximum\": 2.0,\n",
    "                    \"default\": 0.0,\n",
    "                    \"description\": \"Sampling temperature\"\n",
    "                },\n",
    "                \"top_p\": {\n",
    "                    \"type\": \"number\",\n",
    "                    \"minimum\": 0.0,\n",
    "                    \"maximum\": 1.0,\n",
    "                    \"default\": 0.95,\n",
    "                    \"description\": \"Top-p sampling parameter\"\n",
    "                },\n",
    "                \"max_output_tokens\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"minimum\": 1,\n",
    "                    \"maximum\": max_tokens,\n",
    "                    \"default\": max_tokens,\n",
    "                    \"description\": f\"Maximum number of output tokens (max for {current_model}: {max_tokens})\"\n",
    "                },\n",
    "                \"seed\": {\n",
    "                    \"type\": [\"integer\", \"null\"],\n",
    "                    \"default\": None,\n",
    "                    \"description\": \"Random seed for reproducibility\"\n",
    "                },\n",
    "                \"response_mime_type\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"text/plain\", \"application/json\"],\n",
    "                    \"default\": \"text/plain\",\n",
    "                    \"description\": \"Response MIME type\"\n",
    "                },\n",
    "                \"downsample_audio\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": False,\n",
    "                    \"description\": \"Downsample audio before uploading (requires ffmpeg)\"\n",
    "                },\n",
    "                \"downsample_rate\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"enum\": [8000, 16000, 22050, 44100],\n",
    "                    \"default\": 16000,\n",
    "                    \"description\": \"Target sample rate for downsampling\"\n",
    "                },\n",
    "                \"downsample_channels\": {\n",
    "                    \"type\": \"integer\",\n",
    "                    \"enum\": [1, 2],\n",
    "                    \"default\": 1,\n",
    "                    \"description\": \"Number of audio channels (1=mono, 2=stereo)\"\n",
    "                },\n",
    "                \"safety_settings\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"enum\": [\"OFF\", \"BLOCK_NONE\", \"BLOCK_FEW\", \"BLOCK_SOME\", \"BLOCK_MOST\"],\n",
    "                    \"default\": \"OFF\",\n",
    "                    \"description\": \"Safety filter threshold\"\n",
    "                },\n",
    "                \"auto_refresh_models\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": True,\n",
    "                    \"description\": \"Automatically refresh available models list\"\n",
    "                },\n",
    "                \"model_filter\": {\n",
    "                    \"type\": \"array\",\n",
    "                    \"items\": {\"type\": \"string\"},\n",
    "                    \"default\": [],\n",
    "                    \"description\": \"Keywords to exclude from model names (e.g., ['tts', 'image'])\"\n",
    "                },\n",
    "                \"use_file_upload\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": False,\n",
    "                    \"description\": \"Upload audio files to Gemini API instead of embedding in request\"\n",
    "                },\n",
    "                \"use_streaming\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": False,\n",
    "                    \"description\": \"Use streaming response for transcription\"\n",
    "                },\n",
    "                \"delete_uploaded_files\": {\n",
    "                    \"type\": \"boolean\",\n",
    "                    \"default\": True,\n",
    "                    \"description\": \"Delete uploaded files after transcription\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"model\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "\n",
    "    def get_current_config(\n",
    "        self\n",
    "    ) -> Dict[str, Any]:  # Returns the merged configuration dictionary\n",
    "        \"\"\"Return current configuration.\"\"\"\n",
    "        defaults = self.get_config_defaults()\n",
    "        return {**defaults, **self.config}\n",
    "\n",
    "    def initialize(\n",
    "        self,\n",
    "        config: Optional[Dict[str, Any]] = None  # Configuration dictionary to override defaults\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the plugin with configuration.\"\"\"\n",
    "        if config:\n",
    "            is_valid, error = self.validate_config(config)\n",
    "            if not is_valid:\n",
    "                raise ValueError(f\"Invalid configuration: {error}\")\n",
    "        \n",
    "        # Merge with defaults\n",
    "        defaults = self.get_config_defaults()\n",
    "        self.config = {**defaults, **(config or {})}\n",
    "        \n",
    "        # Initialize client\n",
    "        try:\n",
    "            api_key = self._get_api_key()\n",
    "            self.client = genai.Client(api_key=api_key)\n",
    "            \n",
    "            # Refresh available models if enabled\n",
    "            if self.config.get(\"auto_refresh_models\", True):\n",
    "                self.available_models = self._refresh_available_models()\n",
    "                # Update schema with actual available models\n",
    "                schema = self.get_config_schema()\n",
    "                schema[\"properties\"][\"model\"][\"enum\"] = self.available_models\n",
    "            else:\n",
    "                self.available_models = self.DEFAULT_AUDIO_MODELS\n",
    "            \n",
    "            # Update max_output_tokens based on selected model's limit\n",
    "            self._update_max_tokens_for_model(self.config['model'])\n",
    "                \n",
    "            self.logger.info(f\"Initialized Gemini plugin with model '{self.config['model']}'\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to initialize Gemini client: {e}\")\n",
    "            raise\n",
    "\n",
    "    def execute(\n",
    "        self,\n",
    "        audio: Union[AudioData, str, Path],  # Audio data object or path to audio file\n",
    "        **kwargs # Additional arguments to override config\n",
    "    ) -> TranscriptionResult:  # Returns transcription result object\n",
    "        \"\"\"Transcribe audio using Gemini.\"\"\"\n",
    "        if not self.client:\n",
    "            raise RuntimeError(\"Plugin not initialized. Call initialize() first.\")\n",
    "        \n",
    "        # Check if model is being overridden at execution time\n",
    "        if \"model\" in kwargs and kwargs[\"model\"] != self.config.get(\"model\"):\n",
    "            self._update_max_tokens_for_model(kwargs[\"model\"])\n",
    "        \n",
    "        # Prepare audio file\n",
    "        audio_path, temp_created = self._prepare_audio(audio)\n",
    "        uploaded_file = None\n",
    "        \n",
    "        try:\n",
    "            # Merge runtime kwargs with config\n",
    "            exec_config = {**self.config, **kwargs}\n",
    "            \n",
    "            # Decide whether to upload file or embed in request\n",
    "            if exec_config.get(\"use_file_upload\", False):\n",
    "                # Upload audio file to Gemini API\n",
    "                uploaded_file = self._upload_audio_file(audio_path)\n",
    "                audio_content = uploaded_file\n",
    "            else:\n",
    "                # Read audio file and embed in request (existing behavior)\n",
    "                with open(audio_path, 'rb') as f:\n",
    "                    audio_bytes = f.read()\n",
    "                \n",
    "                # Determine MIME type\n",
    "                suffix = audio_path.suffix.lower()\n",
    "                mime_map = {\n",
    "                    '.wav': 'audio/wav',\n",
    "                    '.mp3': 'audio/mp3',\n",
    "                    '.aiff': 'audio/aiff',\n",
    "                    '.aac': 'audio/aac',\n",
    "                    '.ogg': 'audio/ogg',\n",
    "                    '.flac': 'audio/flac'\n",
    "                }\n",
    "                mime_type = mime_map.get(suffix, 'audio/wav')\n",
    "                \n",
    "                # Create audio content part\n",
    "                audio_content = types.Part.from_bytes(\n",
    "                    data=audio_bytes,\n",
    "                    mime_type=mime_type\n",
    "                )\n",
    "            \n",
    "            # Prepare generation config\n",
    "            generate_config = types.GenerateContentConfig(\n",
    "                response_mime_type=exec_config[\"response_mime_type\"],\n",
    "                temperature=exec_config[\"temperature\"],\n",
    "                top_p=exec_config[\"top_p\"],\n",
    "                max_output_tokens=exec_config[\"max_output_tokens\"],\n",
    "                seed=exec_config.get(\"seed\"),\n",
    "                safety_settings=[\n",
    "                    types.SafetySetting(\n",
    "                        category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                        threshold=exec_config[\"safety_settings\"]\n",
    "                    ),\n",
    "                    types.SafetySetting(\n",
    "                        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                        threshold=exec_config[\"safety_settings\"]\n",
    "                    ),\n",
    "                    types.SafetySetting(\n",
    "                        category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                        threshold=exec_config[\"safety_settings\"]\n",
    "                    ),\n",
    "                    types.SafetySetting(\n",
    "                        category=\"HARM_CATEGORY_HARASSMENT\",\n",
    "                        threshold=exec_config[\"safety_settings\"]\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            \n",
    "            # Prepare contents\n",
    "            prompt = exec_config[\"prompt\"]\n",
    "            contents = [prompt, audio_content]\n",
    "            \n",
    "            # Generate transcription\n",
    "            self.logger.info(f\"Transcribing with Gemini model: {exec_config['model']} (max_tokens: {exec_config['max_output_tokens']})\")\n",
    "            \n",
    "            # Use streaming or regular generation\n",
    "            if exec_config.get(\"use_streaming\", False):\n",
    "                # Use streaming response\n",
    "                transcribed_text = \"\"\n",
    "                for chunk in self.client.models.generate_content_stream(\n",
    "                    model=exec_config[\"model\"],\n",
    "                    contents=contents,\n",
    "                    config=generate_config\n",
    "                ):\n",
    "                    if hasattr(chunk, 'text'):\n",
    "                        transcribed_text += chunk.text\n",
    "                        # Could optionally yield chunks here for real-time processing\n",
    "                self.logger.info(\"Streaming transcription completed\")\n",
    "            else:\n",
    "                # Use regular response (existing behavior)\n",
    "                response = self.client.models.generate_content(\n",
    "                    model=exec_config[\"model\"],\n",
    "                    contents=contents,\n",
    "                    config=generate_config\n",
    "                )\n",
    "                transcribed_text = response.text if hasattr(response, 'text') else str(response)\n",
    "            \n",
    "            # Create result\n",
    "            result = TranscriptionResult(\n",
    "                text=transcribed_text.strip(),\n",
    "                confidence=None,  # Gemini doesn't provide confidence scores\n",
    "                segments=None,  # Gemini doesn't provide segments by default\n",
    "                metadata={\n",
    "                    \"model\": exec_config[\"model\"],\n",
    "                    \"temperature\": exec_config[\"temperature\"],\n",
    "                    \"top_p\": exec_config[\"top_p\"],\n",
    "                    \"max_output_tokens\": exec_config[\"max_output_tokens\"],\n",
    "                    \"prompt\": prompt[:100] + \"...\" if len(prompt) > 100 else prompt,\n",
    "                    \"use_file_upload\": exec_config.get(\"use_file_upload\", False),\n",
    "                    \"use_streaming\": exec_config.get(\"use_streaming\", False)\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            self.logger.info(f\"Transcription completed: {len(transcribed_text.split())} words\")\n",
    "            return result\n",
    "            \n",
    "        finally:\n",
    "            # Clean up uploaded file if configured to do so\n",
    "            if uploaded_file and exec_config.get(\"delete_uploaded_files\", True):\n",
    "                try:\n",
    "                    self._delete_uploaded_file(uploaded_file.name)\n",
    "                    # Remove from tracking list\n",
    "                    self.uploaded_files = [f for f in self.uploaded_files if f.name != uploaded_file.name]\n",
    "                except Exception as e:\n",
    "                    self.logger.warning(f\"Failed to cleanup uploaded file: {e}\")\n",
    "            \n",
    "            # Clean up temporary file\n",
    "            if temp_created:\n",
    "                try:\n",
    "                    audio_path.unlink()\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "    def is_available(\n",
    "        self\n",
    "    ) -> bool:  # Returns True if the Gemini API is available\n",
    "        \"\"\"Check if Gemini API is available.\"\"\"\n",
    "        return GEMINI_AVAILABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a46385-4987-4ead-9be7-f0ef0e49e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _get_api_key(\n",
    "    self:GeminiPlugin\n",
    ") -> str:  # Returns the API key string\n",
    "    \"\"\"Get API key from config or environment.\"\"\"\n",
    "    api_key = self.config.get(\"api_key\")\n",
    "    if not api_key:\n",
    "        api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "    if not api_key:\n",
    "        raise ValueError(\"No API key provided. Set GEMINI_API_KEY environment variable or provide api_key in config\")\n",
    "    return api_key\n",
    "\n",
    "@patch\n",
    "def _refresh_available_models(\n",
    "    self:GeminiPlugin\n",
    ") -> List[str]:  # Returns list of available model names\n",
    "    \"\"\"Fetch and filter available models from Gemini API.\"\"\"\n",
    "    try:\n",
    "        if not self.client:\n",
    "            return self.DEFAULT_AUDIO_MODELS\n",
    "        \n",
    "        # Get all models that support content generation\n",
    "        all_models = list(self.client.models.list())\n",
    "        gen_models = [model for model in all_models if 'generateContent' in model.supported_actions]\n",
    "        \n",
    "        # Extract model names and apply filters\n",
    "        model_filter = self.config.get(\"model_filter\", [])\n",
    "        if not model_filter:\n",
    "            model_filter = ['tts', 'image', 'learn']  # Default exclusions\n",
    "        \n",
    "        filtered_names = []\n",
    "        self.model_token_limits = {}  # Reset token limits\n",
    "        \n",
    "        for model in gen_models:\n",
    "            model_name = model.name.removeprefix('models/')\n",
    "            # Skip if any filter keyword is in the model name\n",
    "            if not any(keyword in model_name.lower() for keyword in model_filter):\n",
    "                filtered_names.append(model_name)\n",
    "                # Store the output token limit for this model\n",
    "                self.model_token_limits[model_name] = model.output_token_limit\n",
    "        \n",
    "        # Sort with newest/best models first\n",
    "        filtered_names.sort(reverse=True)\n",
    "        \n",
    "        self.logger.info(f\"Found {len(filtered_names)} audio-capable models\")\n",
    "        return filtered_names if filtered_names else self.DEFAULT_AUDIO_MODELS\n",
    "        \n",
    "    except Exception as e:\n",
    "        self.logger.warning(f\"Could not fetch models from API: {e}. Using defaults.\")\n",
    "        return self.DEFAULT_AUDIO_MODELS\n",
    "\n",
    "@patch\n",
    "def _update_max_tokens_for_model(\n",
    "    self:GeminiPlugin,\n",
    "    model_name: str  # Model name to update tokens for\n",
    ") -> None:\n",
    "    \"\"\"Update max_output_tokens config based on the model's token limit.\"\"\"\n",
    "    if model_name in self.model_token_limits:\n",
    "        token_limit = self.model_token_limits[model_name]\n",
    "        # Update the config with the model's actual limit\n",
    "        self.config[\"max_output_tokens\"] = token_limit\n",
    "        self.logger.info(f\"Updated max_output_tokens to {token_limit} for model '{model_name}'\")\n",
    "    else:\n",
    "        # Use a sensible default if we don't have the limit\n",
    "        default_limit = 65536  # Common default for newer models\n",
    "        self.config[\"max_output_tokens\"] = default_limit\n",
    "        self.logger.info(f\"Using default max_output_tokens of {default_limit} for model '{model_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e50ea0-61eb-42bf-a4e5-3e6780a97b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def update_config(\n",
    "    self:GeminiPlugin,\n",
    "    config: Dict[str, Any]  # New configuration values\n",
    ") -> None:\n",
    "    \"\"\"Update plugin configuration, adjusting max_tokens if model changes.\"\"\"\n",
    "    old_model = self.config.get(\"model\")\n",
    "    \n",
    "    # Update config through parent class or directly\n",
    "    self.config.update(config)\n",
    "    \n",
    "    # If model changed, update max_output_tokens\n",
    "    new_model = self.config.get(\"model\")\n",
    "    if new_model and new_model != old_model:\n",
    "        self._update_max_tokens_for_model(new_model)\n",
    "\n",
    "@patch\n",
    "def _prepare_audio(\n",
    "    self:GeminiPlugin,\n",
    "    audio: Union[AudioData, str, Path]  # Audio data object or path to audio file\n",
    ") -> Tuple[Path, bool]:  # Returns tuple of (processed audio path, whether temp file was created)\n",
    "    \"\"\"Prepare audio file for upload.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (audio_path, is_temp_file)\n",
    "    \"\"\"\n",
    "    temp_created = False\n",
    "    \n",
    "    if isinstance(audio, (str, Path)):\n",
    "        audio_path = Path(audio)\n",
    "    elif isinstance(audio, AudioData):\n",
    "        # Save AudioData to temporary file\n",
    "        import soundfile as sf\n",
    "        temp_file = tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False)\n",
    "        \n",
    "        audio_array = audio.samples\n",
    "        # Convert to mono if stereo\n",
    "        if audio_array.ndim > 1:\n",
    "            audio_array = audio_array.mean(axis=1)\n",
    "        \n",
    "        # Ensure float32 and normalized\n",
    "        if audio_array.dtype != np.float32:\n",
    "            audio_array = audio_array.astype(np.float32)\n",
    "        if audio_array.max() > 1.0:\n",
    "            audio_array = audio_array / np.abs(audio_array).max()\n",
    "        \n",
    "        sf.write(temp_file.name, audio_array, audio.sample_rate)\n",
    "        audio_path = Path(temp_file.name)\n",
    "        temp_created = True\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported audio input type: {type(audio)}\")\n",
    "    \n",
    "    # Optionally downsample audio\n",
    "    if self.config.get(\"downsample_audio\", False) and FFMPEG_AVAILABLE:\n",
    "        try:\n",
    "            downsampled = audio_path.with_stem(f\"{audio_path.stem}_downsampled\")\n",
    "            downsample_audio(\n",
    "                audio_path,\n",
    "                downsampled,\n",
    "                sample_rate=self.config.get(\"downsample_rate\", 16000),\n",
    "                channels=self.config.get(\"downsample_channels\", 1)\n",
    "            )\n",
    "            \n",
    "            # Clean up original temp file if created\n",
    "            if temp_created:\n",
    "                audio_path.unlink()\n",
    "            \n",
    "            audio_path = downsampled\n",
    "            temp_created = True\n",
    "            self.logger.info(f\"Downsampled audio to {self.config['downsample_rate']}Hz\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Failed to downsample audio: {e}\")\n",
    "    \n",
    "    return audio_path, temp_created\n",
    "\n",
    "@patch\n",
    "def _upload_audio_file(\n",
    "    self:GeminiPlugin,\n",
    "    audio_path: Path  # Path to audio file to upload\n",
    ") -> Any:  # Returns uploaded file object\n",
    "    \"\"\"Upload audio file to Gemini API.\n",
    "    \n",
    "    Returns:\n",
    "        Uploaded file object\n",
    "    \"\"\"\n",
    "    try:\n",
    "        self.logger.info(f\"Uploading audio file: {audio_path}\")\n",
    "        uploaded_file = self.client.files.upload(file=audio_path)\n",
    "        self.uploaded_files.append(uploaded_file)  # Track for cleanup\n",
    "        self.logger.info(f\"Successfully uploaded file: {uploaded_file.name}\")\n",
    "        return uploaded_file\n",
    "    except Exception as e:\n",
    "        self.logger.error(f\"Failed to upload audio file: {e}\")\n",
    "        raise\n",
    "\n",
    "@patch\n",
    "def _delete_uploaded_file(\n",
    "    self:GeminiPlugin,\n",
    "    file_name: str  # Name of file to delete\n",
    ") -> None:\n",
    "    \"\"\"Delete an uploaded file from Gemini API.\"\"\"\n",
    "    try:\n",
    "        self.client.files.delete(name=file_name)\n",
    "        self.logger.info(f\"Deleted uploaded file: {file_name}\")\n",
    "    except Exception as e:\n",
    "        self.logger.warning(f\"Failed to delete uploaded file {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0932b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def cleanup(\n",
    "    self:GeminiPlugin\n",
    ") -> None:\n",
    "    \"\"\"Clean up resources.\"\"\"\n",
    "    # Clean up any remaining uploaded files\n",
    "    if self.config.get(\"delete_uploaded_files\", True):\n",
    "        for uploaded_file in self.uploaded_files:\n",
    "            try:\n",
    "                self._delete_uploaded_file(uploaded_file.name)\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Failed to delete file during cleanup: {e}\")\n",
    "    \n",
    "    self.uploaded_files = []\n",
    "    self.client = None\n",
    "    self.logger.info(\"Cleanup completed\")\n",
    "\n",
    "@patch\n",
    "def get_available_models(\n",
    "    self:GeminiPlugin\n",
    ") -> List[str]:  # Returns list of available model names\n",
    "    \"\"\"Get list of available audio-capable models.\"\"\"\n",
    "    if self.config.get(\"auto_refresh_models\", True) and self.client:\n",
    "        self.available_models = self._refresh_available_models()\n",
    "    return self.available_models\n",
    "\n",
    "@patch\n",
    "def get_model_info(\n",
    "    self:GeminiPlugin,\n",
    "    model_name: Optional[str] = None  # Model name to get info for, defaults to current model\n",
    ") -> Dict[str, Any]:  # Returns dict with model information\n",
    "    \"\"\"Get information about a specific model including token limits.\"\"\"\n",
    "    if model_name is None:\n",
    "        model_name = self.config.get(\"model\", \"gemini-2.5-flash\")\n",
    "    \n",
    "    return {\n",
    "        \"name\": model_name,\n",
    "        \"output_token_limit\": self.model_token_limits.get(model_name, 65536),\n",
    "        \"current_max_output_tokens\": self.config.get(\"max_output_tokens\", 65536)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "odr8y9xm0z",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import Generator\n",
    "\n",
    "@patch\n",
    "def supports_streaming(\n",
    "    self:GeminiPlugin\n",
    ") -> bool:  # Returns True if streaming is supported\n",
    "    \"\"\"Check if this plugin supports streaming transcription.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True, as Gemini supports streaming transcription\n",
    "    \"\"\"\n",
    "    return True\n",
    "\n",
    "@patch  \n",
    "def execute_stream(\n",
    "    self:GeminiPlugin,\n",
    "    audio: Union[AudioData, str, Path],  # Audio data object or path to audio file\n",
    "    **kwargs  # Additional arguments to override config\n",
    ") -> Generator[str, None, TranscriptionResult]:  # Yields text chunks, returns final result\n",
    "    \"\"\"Stream transcription results chunk by chunk.\n",
    "    \n",
    "    This method streams transcription chunks in real-time as they are generated\n",
    "    by the Gemini API.\n",
    "    \n",
    "    Args:\n",
    "        audio: Audio data or path to audio file\n",
    "        **kwargs: Additional plugin-specific parameters\n",
    "        \n",
    "    Yields:\n",
    "        str: Partial transcription text chunks as they become available\n",
    "        \n",
    "    Returns:\n",
    "        TranscriptionResult: Final complete transcription with metadata\n",
    "        \n",
    "    Example:\n",
    "        >>> # Stream transcription chunks in real-time\n",
    "        >>> for chunk in plugin.execute_stream(audio_file):\n",
    "        ...     print(chunk, end=\"\", flush=True)\n",
    "    \"\"\"\n",
    "    if not self.client:\n",
    "        raise RuntimeError(\"Plugin not initialized. Call initialize() first.\")\n",
    "    \n",
    "    # Force streaming mode in config\n",
    "    kwargs['use_streaming'] = True\n",
    "    \n",
    "    # Check if model is being overridden at execution time\n",
    "    if \"model\" in kwargs and kwargs[\"model\"] != self.config.get(\"model\"):\n",
    "        self._update_max_tokens_for_model(kwargs[\"model\"])\n",
    "    \n",
    "    # Prepare audio file\n",
    "    audio_path, temp_created = self._prepare_audio(audio)\n",
    "    uploaded_file = None\n",
    "    \n",
    "    try:\n",
    "        # Merge runtime kwargs with config\n",
    "        exec_config = {**self.config, **kwargs}\n",
    "        \n",
    "        # Decide whether to upload file or embed in request\n",
    "        if exec_config.get(\"use_file_upload\", False):\n",
    "            # Upload audio file to Gemini API\n",
    "            uploaded_file = self._upload_audio_file(audio_path)\n",
    "            audio_content = uploaded_file\n",
    "        else:\n",
    "            # Read audio file and embed in request\n",
    "            with open(audio_path, 'rb') as f:\n",
    "                audio_bytes = f.read()\n",
    "            \n",
    "            # Determine MIME type\n",
    "            suffix = audio_path.suffix.lower()\n",
    "            mime_map = {\n",
    "                '.wav': 'audio/wav',\n",
    "                '.mp3': 'audio/mp3',\n",
    "                '.aiff': 'audio/aiff',\n",
    "                '.aac': 'audio/aac',\n",
    "                '.ogg': 'audio/ogg',\n",
    "                '.flac': 'audio/flac'\n",
    "            }\n",
    "            mime_type = mime_map.get(suffix, 'audio/wav')\n",
    "            \n",
    "            # Create audio content part\n",
    "            audio_content = types.Part.from_bytes(\n",
    "                data=audio_bytes,\n",
    "                mime_type=mime_type\n",
    "            )\n",
    "        \n",
    "        # Prepare generation config\n",
    "        generate_config = types.GenerateContentConfig(\n",
    "            response_mime_type=exec_config[\"response_mime_type\"],\n",
    "            temperature=exec_config[\"temperature\"],\n",
    "            top_p=exec_config[\"top_p\"],\n",
    "            max_output_tokens=exec_config[\"max_output_tokens\"],\n",
    "            seed=exec_config.get(\"seed\"),\n",
    "            safety_settings=[\n",
    "                types.SafetySetting(\n",
    "                    category=\"HARM_CATEGORY_HATE_SPEECH\",\n",
    "                    threshold=exec_config[\"safety_settings\"]\n",
    "                ),\n",
    "                types.SafetySetting(\n",
    "                    category=\"HARM_CATEGORY_DANGEROUS_CONTENT\",\n",
    "                    threshold=exec_config[\"safety_settings\"]\n",
    "                ),\n",
    "                types.SafetySetting(\n",
    "                    category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\",\n",
    "                    threshold=exec_config[\"safety_settings\"]\n",
    "                ),\n",
    "                types.SafetySetting(\n",
    "                    category=\"HARM_CATEGORY_HARASSMENT\",\n",
    "                    threshold=exec_config[\"safety_settings\"]\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Prepare contents\n",
    "        prompt = exec_config[\"prompt\"]\n",
    "        contents = [prompt, audio_content]\n",
    "        \n",
    "        # Generate transcription with streaming\n",
    "        self.logger.info(f\"Streaming transcription with Gemini model: {exec_config['model']} (max_tokens: {exec_config['max_output_tokens']})\")\n",
    "        \n",
    "        transcribed_text = \"\"\n",
    "        chunks_yielded = 0\n",
    "        \n",
    "        # Stream chunks as they arrive\n",
    "        for chunk in self.client.models.generate_content_stream(\n",
    "            model=exec_config[\"model\"],\n",
    "            contents=contents,\n",
    "            config=generate_config\n",
    "        ):\n",
    "            if hasattr(chunk, 'text'):\n",
    "                chunk_text = chunk.text\n",
    "                transcribed_text += chunk_text\n",
    "                chunks_yielded += 1\n",
    "                yield chunk_text  # Yield each chunk in real-time\n",
    "        \n",
    "        self.logger.info(f\"Streaming completed: {chunks_yielded} chunks, {len(transcribed_text.split())} words\")\n",
    "        \n",
    "        # Return final result\n",
    "        return TranscriptionResult(\n",
    "            text=transcribed_text.strip(),\n",
    "            confidence=None,  # Gemini doesn't provide confidence scores\n",
    "            segments=None,  # Gemini doesn't provide segments by default\n",
    "            metadata={\n",
    "                \"model\": exec_config[\"model\"],\n",
    "                \"temperature\": exec_config[\"temperature\"],\n",
    "                \"top_p\": exec_config[\"top_p\"],\n",
    "                \"max_output_tokens\": exec_config[\"max_output_tokens\"],\n",
    "                \"prompt\": prompt[:100] + \"...\" if len(prompt) > 100 else prompt,\n",
    "                \"use_file_upload\": exec_config.get(\"use_file_upload\", False),\n",
    "                \"use_streaming\": True,\n",
    "                \"streaming_chunks\": chunks_yielded\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    finally:\n",
    "        # Clean up uploaded file if configured to do so\n",
    "        if uploaded_file and exec_config.get(\"delete_uploaded_files\", True):\n",
    "            try:\n",
    "                self._delete_uploaded_file(uploaded_file.name)\n",
    "                # Remove from tracking list\n",
    "                self.uploaded_files = [f for f in self.uploaded_files if f.name != uploaded_file.name]\n",
    "            except Exception as e:\n",
    "                self.logger.warning(f\"Failed to cleanup uploaded file: {e}\")\n",
    "        \n",
    "        # Clean up temporary file\n",
    "        if temp_created:\n",
    "            try:\n",
    "                audio_path.unlink()\n",
    "            except Exception:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93916e0",
   "metadata": {},
   "source": [
    "## Testing the Plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ed7728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini available: True\n",
      "Plugin name: gemini\n",
      "Plugin version: 1.0.0\n",
      "Supported formats: ['wav', 'mp3', 'aiff', 'aac', 'ogg', 'flac']\n"
     ]
    }
   ],
   "source": [
    "# Test basic functionality\n",
    "plugin = GeminiPlugin()\n",
    "\n",
    "# Check availability\n",
    "print(f\"Gemini available: {plugin.is_available()}\")\n",
    "print(f\"Plugin name: {plugin.name}\")\n",
    "print(f\"Plugin version: {plugin.version}\")\n",
    "print(f\"Supported formats: {plugin.supported_formats}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5088e155",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration properties:\n",
      "  model: Gemini model to use for transcription\n",
      "  api_key: Google API key (defaults to GEMINI_API_KEY env var)\n",
      "  prompt: Prompt for transcription\n",
      "  temperature: Sampling temperature\n",
      "  top_p: Top-p sampling parameter\n"
     ]
    }
   ],
   "source": [
    "# Test configuration schema\n",
    "schema = plugin.get_config_schema()\n",
    "print(\"Configuration properties:\")\n",
    "for prop, details in list(schema[\"properties\"].items())[:5]:\n",
    "    print(f\"  {prop}: {details.get('description', 'No description')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9d5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with model: gemini-2.5-flash\n",
      "\n",
      "Found 34 available models\n",
      "Top 5 models:\n",
      "  - gemma-3n-e4b-it\n",
      "  - gemma-3n-e2b-it\n",
      "  - gemma-3-4b-it\n",
      "  - gemma-3-27b-it\n",
      "  - gemma-3-1b-it\n"
     ]
    }
   ],
   "source": [
    "# Test initialization (requires API key)\n",
    "if os.environ.get(\"GEMINI_API_KEY\"):\n",
    "    plugin.initialize({\"model\": \"gemini-2.5-flash\"})\n",
    "    print(f\"Initialized with model: {plugin.config['model']}\")\n",
    "    \n",
    "    # Get available models\n",
    "    models = plugin.get_available_models()\n",
    "    print(f\"\\nFound {len(models)} available models\")\n",
    "    print(\"Top 5 models:\")\n",
    "    for model in models[:5]:\n",
    "        print(f\"  - {model}\")\n",
    "else:\n",
    "    print(\"Set GEMINI_API_KEY environment variable to test initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sjochf4ca3",
   "metadata": {},
   "source": [
    "## Testing Dynamic Token Limits\n",
    "\n",
    "Test that max_output_tokens is dynamically updated based on the selected model's output_token_limit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pyceosfd71a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token limits for different models:\n",
      "--------------------------------------------------\n",
      "gemini-2.5-pro-preview-03-25: 65,536 tokens\n",
      "gemini-2.5-flash-preview-05-20: 65,536 tokens\n",
      "gemini-2.5-flash: 65,536 tokens\n",
      "gemini-2.5-flash-lite-preview-06-17: 65,536 tokens\n",
      "gemini-2.5-pro-preview-05-06: 65,536 tokens\n",
      "\n",
      "Current configuration:\n",
      "Model: gemini-2.5-flash\n",
      "Max output tokens: 65,536\n",
      "\n",
      "Model info for gemini-2.5-flash:\n",
      "  Output token limit: 65,536\n",
      "  Current max_output_tokens: 65,536\n"
     ]
    }
   ],
   "source": [
    "# Test dynamic token limit updates\n",
    "if os.environ.get(\"GEMINI_API_KEY\"):\n",
    "    # Initialize plugin\n",
    "    plugin = GeminiPlugin()\n",
    "    plugin.initialize({\"model\": \"gemini-2.5-flash\"})\n",
    "    \n",
    "    # Check token limits for different models\n",
    "    print(\"Token limits for different models:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Display token limits that were discovered\n",
    "    for model_name in list(plugin.model_token_limits.keys())[:5]:\n",
    "        token_limit = plugin.model_token_limits[model_name]\n",
    "        print(f\"{model_name}: {token_limit:,} tokens\")\n",
    "    \n",
    "    print(\"\\nCurrent configuration:\")\n",
    "    print(f\"Model: {plugin.config['model']}\")\n",
    "    print(f\"Max output tokens: {plugin.config['max_output_tokens']:,}\")\n",
    "    \n",
    "    # Get model info\n",
    "    model_info = plugin.get_model_info()\n",
    "    print(f\"\\nModel info for {model_info['name']}:\")\n",
    "    print(f\"  Output token limit: {model_info['output_token_limit']:,}\")\n",
    "    print(f\"  Current max_output_tokens: {model_info['current_max_output_tokens']:,}\")\n",
    "else:\n",
    "    print(\"Set GEMINI_API_KEY environment variable to test token limits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dgspoh6l7ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model switching and token limit updates:\n",
      "--------------------------------------------------\n",
      "\n",
      "Switched to model: gemini-2.5-flash\n",
      "  Token limit: 65,536\n",
      "  Config max_output_tokens: 65,536\n",
      "  Schema maximum: 65,536\n",
      "  Schema default: 65,536\n",
      "\n",
      "Switched to model: gemini-2.0-flash\n",
      "  Token limit: 8,192\n",
      "  Config max_output_tokens: 8,192\n",
      "  Schema maximum: 65,536\n",
      "  Schema default: 65,536\n"
     ]
    }
   ],
   "source": [
    "# Test switching models and automatic token limit update\n",
    "if os.environ.get(\"GEMINI_API_KEY\"):\n",
    "    # Switch to a different model\n",
    "    print(\"Testing model switching and token limit updates:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    test_models = [\"gemini-2.5-flash\", \"gemini-1.5-pro\", \"gemini-2.0-flash\"]\n",
    "    \n",
    "    for model_name in test_models:\n",
    "        if model_name in plugin.model_token_limits:\n",
    "            # Update configuration with new model\n",
    "            plugin.update_config({\"model\": model_name})\n",
    "            \n",
    "            print(f\"\\nSwitched to model: {model_name}\")\n",
    "            print(f\"  Token limit: {plugin.model_token_limits[model_name]:,}\")\n",
    "            print(f\"  Config max_output_tokens: {plugin.config['max_output_tokens']:,}\")\n",
    "            \n",
    "            # Verify schema is updated\n",
    "            schema = plugin.get_config_schema()\n",
    "            max_tokens_prop = schema[\"properties\"][\"max_output_tokens\"]\n",
    "            print(f\"  Schema maximum: {max_tokens_prop['maximum']:,}\")\n",
    "            print(f\"  Schema default: {max_tokens_prop['default']:,}\")\n",
    "else:\n",
    "    print(\"Set GEMINI_API_KEY environment variable to test model switching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "areeptnw8y",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing runtime model override:\n",
      "--------------------------------------------------\n",
      "Current model: gemini-2.0-flash\n",
      "Current max_output_tokens: 8,192\n",
      "\n",
      "Executing with override model: gemini-2.5-flash\n",
      "Expected token limit: 65,536\n",
      "\n",
      "Transcription metadata:\n",
      "  Model used: gemini-2.5-flash\n",
      "  Max output tokens: 65,536\n",
      "\n",
      "Config after execution:\n",
      "  Model: gemini-2.0-flash\n",
      "  Max output tokens: 65,536\n"
     ]
    }
   ],
   "source": [
    "# Test execution with runtime model override\n",
    "if os.environ.get(\"GEMINI_API_KEY\"):\n",
    "    print(\"Testing runtime model override:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Create test audio\n",
    "    import numpy as np\n",
    "    from cjm_transcription_plugin_system.core import AudioData\n",
    "    \n",
    "    test_audio = AudioData(\n",
    "        samples=np.random.randn(16000).astype(np.float32) * 0.1,\n",
    "        sample_rate=16000,\n",
    "        duration=1.0,\n",
    "        filepath=None,\n",
    "        metadata={}\n",
    "    )\n",
    "    \n",
    "    # Current model and token limit\n",
    "    print(f\"Current model: {plugin.config['model']}\")\n",
    "    print(f\"Current max_output_tokens: {plugin.config['max_output_tokens']:,}\")\n",
    "    \n",
    "    # Execute with a different model at runtime\n",
    "    override_model = \"gemini-2.0-flash\" if plugin.config['model'] != \"gemini-2.0-flash\" else \"gemini-2.5-flash\"\n",
    "    \n",
    "    if override_model in plugin.model_token_limits:\n",
    "        print(f\"\\nExecuting with override model: {override_model}\")\n",
    "        print(f\"Expected token limit: {plugin.model_token_limits[override_model]:,}\")\n",
    "        \n",
    "        try:\n",
    "            result = plugin.execute(\n",
    "                test_audio,\n",
    "                model=override_model,\n",
    "                prompt=\"This is a test audio signal.\"\n",
    "            )\n",
    "            \n",
    "            print(f\"\\nTranscription metadata:\")\n",
    "            print(f\"  Model used: {result.metadata['model']}\")\n",
    "            print(f\"  Max output tokens: {result.metadata['max_output_tokens']:,}\")\n",
    "            \n",
    "            # Check if config was updated\n",
    "            print(f\"\\nConfig after execution:\")\n",
    "            print(f\"  Model: {plugin.config['model']}\")\n",
    "            print(f\"  Max output tokens: {plugin.config['max_output_tokens']:,}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Execution error (expected for random audio): {e}\")\n",
    "else:\n",
    "    print(\"Set GEMINI_API_KEY environment variable to test runtime override\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a52dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
